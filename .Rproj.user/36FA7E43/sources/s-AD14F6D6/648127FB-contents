rm(list=ls())   # clear out old junk then read in data

#first load some needed libraries
library("psych")
library("lessR")

####################  READ IN DATA AND DELETE MISSING #########################
#set working directory and read in datafile
d.0 <- read.csv("Data.csv", header=T)
d.1 <- subset(d.0,Finished == 1)  # delete did not finish people
temp <- names(d.1) %in% c("cons.11") 
d <- d.1[!temp]
rm(temp,d.0,d.1)
names(d)
d.1 <- d[complete.cases(d),]
recoded.d <- recode(c(Friend.12,Frugal.20,cons.7:cons.10), old=1:5, new=5:1, data=d.1)

# check for reverse coding
# alpha(recoded.d[cons.vars])
# alpha(recoded.d[friend.vars])
# alpha(recoded.d[frug.vars])
# alpha(recoded.d[pain.vars])
# alpha(recoded.d[tight.vars])


friend.vars <- grep("Friend.",names(d),value=TRUE)
pain.vars <-  grep("Pain.",names(d),value=TRUE)
tight.vars <-  grep("Tight.",names(d),value=TRUE)
cons.vars <-  grep("cons.",names(d),value=TRUE)
frug.vars <-  grep("Frugal.",names(d),value=TRUE)


# parse scales into individual dataframes
d.cons <- recoded.d[cons.vars]
head(d.cons)
d.friend <- recoded.d[friend.vars]
head(d.friend)
d.frug <- recoded.d[frug.vars]
head(d.frug)
d.pain <- recoded.d[pain.vars]
head(d.pain)
d.tight <- recoded.d[tight.vars]
head(d.tight)

d.final <- recoded.d
rm(d,recoded.d,d.1)

#compute scale scores for later
d.final$cons <- rowMeans(d.final[cons.vars])
d.final$friend <- rowMeans(d.final[friend.vars])
d.final$frug <- rowMeans(d.final[frug.vars])
d.final$pain <- rowMeans(d.final[pain.vars])
d.final$tight <- rowMeans(d.final[tight.vars])

names(d.final)
keep.var <- c("cons","friend","frug","pain","tight")
d.scales <- d.final[keep.var]
names(d.scales)
#####################################################################################
#  START IRT STUFF
#####################################################################################

###### STEP 1 - Test assumptions
# Run a parallel analysis here. also run EFA and look at loadings
# tight scale
# 1. If you take good care of your possessions, you will definitely save money in the long run.
# 2. There are many things that are normally thrown away that are still quite useful.
# 3. Making better use of my resources makes me feel good.
# 4. If you can reuse an item you already have, there's no sense in buying something new.
# 5. I believe in being careful in how I spend my money.
# 6. I discipline myself to get the most from my money.
# 7. I am willing to wait on a purchase I want so that I can save money.
# 8. There are things I resist buying today so I can save for tomorrow.
# 

d <- d.tight     # specify data so we can just reuse code below
fa.parallel(d)

# oblique rotation
efa.1 <- fa(d,nfactors=1,rotate="oblimin")
print(efa.1, sort=TRUE)
efa.1$values  # this is the EFA eigenvalues.  REPORT THESE
efa.1$loadings
fa.diagram(efa.1)


# double-check 2 factor solution
efa.2 <- fa(d,nfactors=2,rotate="oblimin")
print(efa.2, sort=TRUE)
efa.2$values  # this is the EFA eigenvalues.  REPORT THESE
efa.2$loadings
fa.diagram(efa.2)
rm(efa.1,efa.2)
#####  END STEP 1


apply(d,2,table)  

table(d$Tight.1)

##### STEP 2 - Run GRM
library("mirt")
?mirt
out.grm <- mirt(d, model = 1, itemtype = "graded", SE=TRUE) 
out.grm


### STEP 3- Check model fit
?itemfit
fit <- itemfit(out.grm, X2 = TRUE)
fit
M2(out.grm)
plots <- list()
for(i in 1:length(d)){
  plots[[i]]<-itemfit(out.grm,empirical.plot = i)
}
plots

#### Step 4 - look at model
plot(out.grm)  # expected test scores
plot(out.grm, type="info")  #test info
plot(out.grm, type="infoSE") #item info
plot(out.grm, type="trace") #item CRCs
plot(out.grm, type="infotrace") #item info

plots <- list()
for(i in 1:length(d)){
  plots[[i]]<-itemplot(out.grm,i)
}
plots

# difficulty (b) = -d/a 
(coef.table <- coef(out.grm, simplify = TRUE, IRTpars = TRUE)[[1]])
coef(out.grm, printSE=TRUE)
coef(out.grm, IRTpars=TRUE, printSE=TRUE)  # so just grab SE from non-IRT format
### NOTE you get DIFFERENT SEs for IRT parameters than default

###### Step 5 - Collapse categories
# It's a good idea to collapse categories when SE > .5

##### If you do have to do some recoding, here is where you do it.
apply(d,2,table)  #apply to data "d", do for all columns ("2" rather than 1=rows), apply the "table" function
for(i in 1:ncol(d)){
  c.name <- colnames(d[i])
  hist(d[,i], main=c.name)
}
rm(c.name,i)
names(d)

### you might need multiple recodes to do this
recoded.d <- recode(c(Tight.1:Tight.8), old=1:5, new=c(0,0,0,1,2), data = d)
apply(recoded.d,2,table)  

#ideally you just recode the items that most need it.  i've been using rule of thumb n<20


######  Step 5 - now re-run your analyses